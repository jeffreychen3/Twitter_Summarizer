# NETS213 Final Project - HASHTAG DICTIONARY - a Crowdsourced Hashtag Summarizer 

**Component 1: Selecting top-trending hashtags from Twitter’s trending page that have been active for more than 24 hrs.** *(2 points)*

For this component, we planned to select 10 hashtags from the Twitter trending page. We used newly created twitter accounts, that weren’t following anything, to view the trending page on Twitter, as we did not want accounts we followed to affect what we saw on the trending page. However, it was inevitable that our location affected what we saw on the trending page, which was okay, as we were more inclined to select English hashtags in general, rather than ones with tweets primarily in other languages. Note that Twitter states on their website that, “Trends are determined by an algorithm and, by default, are tailored for you based on who you follow, your interests, and your location.” Thus, it is possible that Twitter has collected data from sources besides just one’s location and what they follow on Twitter; thus, it may be impossible for us to truly see a generic Twitter trending page. We thought about using online data sites to track trending hashtags, but realized that the point of this project is not to select the top trending hashtags, but to simply select trending ones that may be obscure or unfamiliar to people, and thus require some sort of summary. 

We came up with selection criteria when choosing our 10 hashtags. Note that the hashtags are being chosen over a period of about 3 weeks (~3 hashtags per week), in order to create a timeline at the end of our project that displays crowdsourced summaries along with hashtags that were trending at points in our projects. The criteria we came up with was as follows. 1. Select hashtags that have been trending for more than 24 hrs. Once we spot a trending hashtag, we may check tweets in the hashtag to see if it has been very active for more than 24 hrs, or simply look up the news associated with that hashtag (example #Myanmar, related to the military coup in the country) and see how long ago the news came out.  2. Select hashtags with primarily English tweets. 3. Select hashtags that aren’t obvious as to what they are about. For example, #Biden2020 would be a bit too obvious as almost everyone knows what such a hashtag would be about. However, #dogecoin is not as obvious to many people, especially those not involved in cryptocurrency, finance, or part of gen-z. Selecting these types of hashtags makes the summarizing process more interesting, as we want to see varying results based on how much information/context (more/less/no tweets) we provide to workers to write their summaries.

Lastly, note that we aimed to select about 3-5 hashtags for which Twitter had a summary for. As will be seen in later components, one of the metrics we wanted to look at was quality ratings for these Twitter generated summaries (see #HobbyLobby below) versus the summaries generated by crowdsourcing.

<img width="613" alt="Screen Shot 2021-04-13 at 12 58 03 PM" src="https://user-images.githubusercontent.com/56012430/114591484-ed2a6b00-9c57-11eb-843e-359686b53991.png">



**Component 2: Online-bot tweet-to-CSV service: provides a file with ~200 tweets, specifically ones associated to a given hashtag and with a minimum of 100 likes.** *(2 points)*
	
We used the following Twitter bot to extract about 200 tweets from the hashtags that we had selected: https://seobots.io/bots/twitter-hashtag-scraper. This bot was cheap to use, and gave us the option to determine the number of tweets we wanted to scrape for in each hashtag, the language that the tweet needed to be in, and the minimum number of likes that a tweet needed to have. We selected a minimum of 25 likes so as to make sure that the tweets we were getting were representative of what most people interacting with the hashtag were actually interacting with. If we had set this barrier to zero, it is possible that we scraped tweets that were just “noise”—spam, or unrelated to the hashtag itself. This bot runs with a completion time of under two minutes. = It’s worth noting that the csv generated from this bot has columns for the following data: Username, User Handle, Date of Posting, Tweet Text, ReTweet Count, Like Count.

<img width="1369" alt="Screen Shot 2021-04-13 at 12 55 58 PM" src="https://user-images.githubusercontent.com/56012430/114591179-9e7cd100-9c57-11eb-970f-8bb802458d0e.png">

**Component 3: Generation 1 of HITs to generate summaries via crowdsourcing about why a particular hashtag is trending. Three different batches of HITs per hashtag for varying sources of information given to workers (No Tweets/Google News Only, 10 Tweets, 20 Tweets)** *(4 points)*

We will post three different HITs to mTurk. The purpose of such is to have varying amounts of information for the summary,  and to be eventually able to compare the quality of the summary based on how much further research is required from the information given. For one HIT we ask the workers to look up the hashtag on Google News only. To incorporate more information to help workers to explain the trendiness of some hashtag, the workers in other two HITs are given 10 tweets or 20 tweets pulled from the scraped tweets csv and allowed to look up on Google News. 

To create the HITs all in one csv (we have three CSVs, one with no tweets per HIT, one with 10 per HIT, one with 20 per HIT), we will first need to aggregate the tweets of different hashtags from our twitter-scraping bot. One HIT (one line of our CSVs) only contains tweets pertaining to one hashtag, however we do not want workers receiving HITs that pertain to the same hashtag. Thus, we will set a limit on the number of tasks workers can do (about 3-5) to ensure that we minimize the number of workers looking at the same hashtag twice. In addition, we will assign the tweets to HITs such that they are grouped with tweets that have a similar number of likes (so some HITs will have tweets with a lot of likes, while others may not be as many). This is such that we can later identify if there is correlation between popular tweets and the quality of the summary. 
In all HITs, workers will be informed about the hashtag. The result of each HIT will be aggregated into one file, and manipulated with python to create the generation 2 HITS.

**Component 4: QC: Generation 2 of HITs to use crowdsourcing as quality control, where three workers workers give ratings to each summary, allowing for averaged ratings to be generated** *(4 points)*

We will implement another three separate HITs each containing summaries from No tweet, 10 Tweets and 20 Tweets groups. Each summary will be given to three workers for rating between 1-5 along with the associated hashtags. We will give criteria for each rating in our instruction to avoid some noise due to different people’s standards (see the mockup hits for the criteria). After each HIT being completed, we will average out the score for each summary using python. 


**Component 5: Aggregation Module where various graphs comparing ratings in our 3 different categories (Google News, 10 tweets, 20 tweets) of summaries will be generated.** *(4 points)*

We will now be left with 4 CSVs. Each CSV will contain averaged ratings for summaries, and each CSV will pertain to one of the following categories: workers given no tweets and solely told to use Google News, workers given 10 tweets, and workers given 20 tweets. We want to now aggregate these 4 CSVs and use them to present some summarized data. The first set of data we want to present is a line graph of average summary rating vs information given, where we consider the least information -> most information given to workers to be in the following order: Google News, workers given 10 tweets, workers given 20 tweets. We also want to present a graphic that shows the highest rated summary for each hashtag, along with which of our 3 categories the summary was in. Lastly, we want to analyze if giving workers more popular tweets in the 10 tweet/20 tweet category actually helped generate better summaries or not. We will thus create a graph of average likes versus summary rating, where average likes is the averaged like count for the tweets given to a worker that generated a given summary (recall that our bot collects data on likes for each tweet). 

**Component 6: Aggregation Module Part 2, where the best summaries are selected from each hashtag based on ratings from workers.** *(1 point)*

The requesters will take the top scoring summary for each hashtag among the three crowdsourced generated categories (no tweets, 10 tweets, 20 tweets), and display the best summary for each of the 10 hashtags; this will show the final results of the crowdworkers work in generating a summary about why a hashtag is trending. 


Point Total: 19 points

-------------------------------------------------------
# PATHS TO DELIVERABLE 2/3 ITEMS:

**How to do HITS**

See the document *HIT Instruction.md* for more information on how to do our demonstration of HITs.
Note that our "real" data was collected on the actual MTurk website, not the sandbox.

**RAW DATA** - *data/RawTweetScrapes/*

See "Raw Data Format and Tweet Selection Discussion" for details

**FIRST SET OF HITS**
*data/SampleData/0Tweets/0TweetsRaw*, *data/SampleData/0Tweets/10TweetsRaw*, *data/SampleData/0Tweets/20TweetsRaw* 
This is the sample csvs for the first set of HITs we will be running, where workers are given sets of tweets and a hashtag that those tweets correlate to, and are asked to write a summary about why the hashtag is trending using the information from those tweets as well as supplementary research.

**Sample input/output for QC** - *data/SampleData/0Tweets/0TweetsQCInput.csv, data/SampleData/0Tweets/0TweetsQCOutput.csv, data/SampleData/10Tweets/10TweetsQCInput.csv, data/SampleData/10Tweets/10TweetsQCOutput.csv, data/SampleData/20Tweets/20TweetsQCInput.csv, data/SampleData/20Tweets/20TweetsQCOutput.csv*

The QC input files contain the data from the first batch of HITs, where workers wrote summaries. Each row of this file is a summary that was written by each worker along with the tweets the worker was given and the average like count of these tweets. The QC output files contain 3x as many rows, because when we run the QCinput files as HITs on MTurk, we will have 3 workers giving ratings to one summary, so there are 3 ratings per summary.

**Sample input/output for Aggregation** - See input at *data/SampleData/0Tweets/0TweetsAggregationPrep.csv, data/SampleData/10Tweets/10TweetsAggregationPrep.csv, data/SampleData/20Tweets/20TweetsAggregationPrep.csv* - See output at *AggregationScreenshots/*

The Aggregation Prep files is data taken from manipulated QC output files, where the average rating for each summary was calculated and thus we return to only having one row per summary and not 3 rows. These files are used to create various graphs and diagrams which can be seen in the AggregationScreenshots/ folder.

**Code for HTML MTurk HITs**

*src/MTurk/* In order to get the HITS into the form of a CSV file, we had to take the raw dataa that was given from the bot. We only wanted the first 200 tweets with only the hashtag, text, and number of likes from each CSV file so we first had to strip the tweets to only have the data we wanted. Then we created a dataframe that stored all the tweets from all the CSV files and we created three unique CSVs based on the number tweets that need to be given. For example, for the 20tweets CSV file, there would be 20 tweets that are given per HIT. We also averaged the number of likes per set of tweets per HITs.  

**Code for QC**

*src/QCCode/AggPrepAndSelectHighest.py*
Code Explanation: 
For each individual input csv, we iterate all the rows. For each set of three rows(each summary will have three ratings) we keep track of their sum and at the last row, we append the row to a new dataframe with 'rating' column value changed to 'averaged rating'. We output the new dataframe as a new csv. 

Sample outputs:
10TweetsQCOutput.csv to generate 10TweetsAggregationPrep.csv
![10TweetAggregationPrepImg](https://user-images.githubusercontent.com/73623005/115484735-ddc5a600-a285-11eb-9493-ed76d77e2d89.png) 

**Code for Data Aggregation** 

*src/QCCode/AggPrepAndSelectHighest.py* 

Code Explanation: 
We initialize a dictionary variable which use hashtag as key, avg rating, summary and label(which of the 0/10/20 csv file the summary comes from) as value. From the three AggregationPrep dataframes (read from the csvs) we will traverse through each of 3 converted dataframes and change the dictionary value when we find a summary with a higher rating under the corresponding hashtag. This dicitonary will essentially end up displaying the following information for EACH hashtag: highest rated summary, number of tweets with which this summary was written with (0, 10, 20), average likes of these tweets.

(the printout is shown at the bottom of the image)
![highestRatedTagList](https://user-images.githubusercontent.com/73623005/115456556-df777580-a255-11eb-86a2-8ada472ff413.png)

*src/AggregationCode/QualityvsAmountData.py* creates a graph of Quality vs Amount of Data (0, 10, 20 tweets) that can be seen in the screenshots folder.

*src/AggregationCode/RatingvsLikes.py* creates a graph of Average Rating vs Average Likes (that is, the average number of likes of the tweets given to a worker who wrote the summary corresponding to a particular rating) that can be seen in the screenshots folder. Note that this graph shows us whether giving workers more popular tweets helps them to write more accurate/highly rated summaries.

**Sample Data Returned**

This is an example of crowdsourced data on the regular MTurk website for the 0-Tweet HITs.
![Screen Shot 2021-04-27 at 5 24 51 PM](https://user-images.githubusercontent.com/56012430/116314889-b4bd7d80-a77d-11eb-9f17-f5e427d13c30.png)


-------------------------------------------------------
# Raw Data Format and Tweet Selection Discussion

We collected our data using the bot https://seobots.io/bots/twitter-hashtag-scraper. The bot generates one CSV per hashtag with columns for the following data: Username, User Handle, Date of Posting, Tweet Text, ReTweet Count, Like Count. Note that this bot-sourced data is not returned in a csv format, where commas are used to separate data, but rather in a space-separated format (we convert this data to a csv version however). We paid (tbd, cost still increasing as we collect more data) to use the bot, which allowed us to determine the number of tweets we wanted to scrape for in each hashtag, the language that the tweet needed to be in, and the minimum number of likes that a tweet needed to have. We chose to only include tweets that had a minimum of 25 likes in order to ensure that the tweets were representative of what most people interacting with the hashtag were actually interacting with. If we had set this barrier to zero, it is possible that we scraped tweets that were just “noise”—spam, or unrelated to the hashtag itself. This bot runs with a completion time of under two minutes, and we sourced at minimum 200 tweets from each hashtag, only using the 200 most popular ones in our actual HITs. 

*#DaunteWright*: Daunte Wright: On April 11, 2021, a Black man named Daunte Wright was pulled over by police officers for a traffic violation related to expired registration tags. According to police, they discovered there was a warrant out for his arrest, so they attempted to detain him. However, this prompted a struggle wherein Wright stepped back into his car and ultimately was killed by a police officer. As he was pulled over, Wright called his mother. He told her that he had been pulled over due to air fresheners hanging on his rearview mirror.  The following day police released the bodycam footage from the event which seemingly indicates that the police officer intended to Tase Wright, but accidentally used her gun. She resigned from the force the next day, along with the Chief of the Brooklyn Center Police Department. The police officer was subsequently arrested and it was announced that she would be charged with second-degree manslaughter. In response, protestors gathered outside of the Brooklyn Center Police Department building. These protests were labeled “unlawful” by law enforcement officials, resulting in police officers firing projectiles into the crows in an effort to get them to disperse. This occurred in the middle of the trial of Derek Chauvin, a police officer who murdered George Floyd just a few miles from where Daunte Wright was killed, as well as increased focus on the over-policing of Black communities and the intense, often deadly, violence that Black people experience from police officers. 

*#EurpoeanSuperLeague*: European Super League: A group of 12 soccer clubs belonging to some of Europe’s biggest soccer leagues announced their plans to form a new group called the Super League. If the League is formed, it would allow some of the biggest soccer clubs in the world permanent spots and play matches midweek; this allows the member-clubs to retain their membership in their domestic competitions. FIFA and UEFA, governing bodies for international and European soccer, respectively, both announced that they opposed the formation of the Super League, and have warned players that they would not be able to participate in FIFA or UEFA-affiliated matches if they choose to go forward with the Super League. 

*#MarsHelicopter*: Mars Helicopter: The American space agency successfully flew a small helicopter, Ingenuity, on Mars. This is the first powered, controlled flight by an aircraft on another planet. Mars’s atmosphere is just 1% of the density on Earth, meaning that flight is difficult. 

*#Myanmar*: Myanmar: Protests have been taking place across Myanmar ever since the military seized control on February 1 following the victory of  Aung San Suu Kyi’s NLD won in a general election. Elected leaders Aung San Suu Kyi and members of her National League for Democracy have been detained, and hundreds of citizens, including children, have been killed by the military. Armed forces have backed the opposition and are demanding a rerun of the vote, with claims of widespread fraud. Now, Min Aung Hlaing, the leader of the coup, is in power. Protests have resulted.

*#Doyoung*: Doyoung: Doyoung is a South Korean singer and vocalist of NCT U and NCT 127. Recent news has surfaced regarding Doyoung’s July musical debut, where he will play the character of Count Axel von Fersen in ‘Marie Antoinette.’  

*#ArmenianGenocide*

*#GEICO500*

*#BETREmembersDMX*

*#supermoon*
